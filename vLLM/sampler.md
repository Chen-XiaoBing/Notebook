### 参考文献：

- [vllm代码走读(五）--sampler](https://zhuanlan.zhihu.com/p/704634078)

### temperature作用补充：

代码：https://github.com/vllm-project/vllm/blob/v0.7.3/vllm/model_executor/layers/sampler.py#L271

在大型语言模型（LLM）中，temperature 是一个重要的超参数，用于控制生成结果的随机性和创造性。具体来说，它影响模型输出概率分布的“平滑”程度，从而调节生成的文本是更倾向于确定性（可预测）还是随机性（多样化）。以下是详细解释：

#### Temperature 的作用原理

LLM 在生成文本时，会为下一个词计算一个概率分布（通常基于 softmax 函数）。temperature 通过调整这个分布的“形状”来影响结果：

公式：假设模型输出的原始 logits（未归一化的得分）为 $z_i$ ,经过 softmax 转换为概率 $p_i$ 时，加入 temperature $T$ 的计算为：

$$ p_i = \frac{exp(z_i/T)}{\sum_j(exp(z_j/T))} $$

- T 较大时：指数函数的差异被缩小，概率分布变得更平滑，各选项的概率更接近，生成结果更随机。
- T 较小时：指数函数的差异被放大，高概率选项被进一步突出，低概率选项被压制，生成结果更确定。

#### Temperature 的取值与效果

- $T < 1$ （如 0.7、0.5）：
    - 降低随机性，模型更倾向于选择高概率的词。
    - 输出更聚焦、更连贯，适合需要准确性或逻辑性的任务（如回答事实性问题）。
    - 极端情况：当 $T -> 0$ 时，模型几乎总是选择概率最高的词，输出变得完全确定。
- $T = 1$ （default value）：
    - 不对概率分布做额外调整，保持模型训练时的原始行为。
    - 随机性与连贯性之间的平衡，适合大多数通用场景。
- $T > 1$ （如 1.5、2.0）：
    - 增加随机性，模型更可能选择低概率的词。
    - 输出更具创造性、多样性，但可能牺牲连贯性，适合生成创意内容（如故事、诗歌）。

#### 实际应用中的控制

- 低 Temperature：用于需要保守、可靠输出的场景，比如技术文档生成或客服回复。
- 高 Temperature：用于需要发散性思维的场景，比如头脑风暴或艺术创作。
- 动态调整：一些应用会根据上下文动态调整 $T$ , 例如在对话中前期用高 $T$ 探索可能性，后期用低 $T$ 收敛到最佳答案。

#### 与其他参数的配合

- Top-k Sampling：只从概率最高的前 $k$ 个词中采样，与 temperature 结合可以限制过于随机的输出。
- Top-p Sampling（Nucleus Sampling）：只从累积概率达到 $p$ 的最小词集采样，进一步平衡随机性和质量。